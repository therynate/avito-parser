from bs4 import BeautifulSoup
import httpx
from concurrent.futures import ThreadPoolExecutor
import threading


try:
    with open("profiles.txt", "r", encoding="utf-8") as f:
        links = [line.strip() for line in f if line.strip()]
except FileNotFoundError:
    print("‚ùå –§–∞–π–ª profiles.txt –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    links = []


try:
    with open("proxies.txt", "r", encoding="utf-8") as f:
        proxies = [line.strip() for line in f if line.strip()]
except FileNotFoundError:
    print("‚ö†Ô∏è –§–∞–π–ª proxies.txt –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    proxies = []


if len(proxies) < len(links):
    print("‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ –º–µ–Ω—å—à–µ, —á–µ–º —Å—Å—ã–ª–æ–∫. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –±–µ–∑ –ø—Ä–æ–∫—Å–∏.")


headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'accept-encoding': 'gzip, deflate, br, zstd',
    'accept-language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/137.0.0.0 Safari/537.36'
}


lock = threading.Lock()


def process_url(args):
    url, proxy = args
    mounts = {}

    if proxy:
        if not proxy.startswith("http://"):
            proxy_url = f"http://{proxy}"
        else:
            proxy_url = proxy

        transport = httpx.HTTPTransport(proxy=proxy_url, verify=False)
        mounts = {
            "http://": transport,
            "https://": transport,
        }

    try:
        with httpx.Client(headers=headers, mounts=mounts, timeout=15) as client:
            response = client.get(url)

            if response.status_code != 200:
                print(f"‚ùå –û—à–∏–±–∫–∞ {response.status_code} –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
                return

            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.find_all("div", {"class": "iva-item-root-Kcj9I"})

            if not items:
                print(f"‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {url} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤.")
                return

            result = [f"–°—Å—ã–ª–∫–∞: {url}\n"]

            for item in items:
                try:
                    name_tag = item.find("div", class_="iva-item-title-mxG7F")
                    price_tag = item.find("span", class_="styles-module-size_xm-RKzt0")

                    name = name_tag.text.strip() if name_tag else "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
                    price = price_tag.text.strip() if price_tag else "–¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

                    print(f"[‚úì] {name} ‚Äî {price}")
                    result.append(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {name}\n–¶–µ–Ω–∞: {price}\n{'-'*40}\n")
                except Exception as e:
                    print("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —ç–ª–µ–º–µ–Ω—Ç–∞:", e)
                    continue

            result.append("="*80 + "\n\n")

            with lock:
                with open("results.txt", "a", encoding="utf-8") as out:
                    out.writelines(result)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url} —Å –ø—Ä–æ–∫—Å–∏ {proxy}: {e}")


if __name__ == "__main__":
    if not links:
        print("–ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏. –í—ã—Ö–æ–¥.")
        exit()

    tasks = [(links[i], proxies[i] if i < len(proxies) else None) for i in range(len(links))]

    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(links)} –ø—Ä–æ—Ñ–∏–ª–µ–π...")

    with ThreadPoolExecutor(max_workers=min(10, len(links))) as executor:
        executor.map(process_url, tasks)

    print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª–µ results.txt.")
